<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="CMSC423" />
  <title>Final Notes on classification</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../css/lecture_style.css" type="text/css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">MathJax.Hub.Queue(["Typeset",MathJax.Hub]);</script>
</head>
<body>
<div id="header">
<h1 class="title">Final Notes on classification</h1>
<h2 class="author">CMSC423</h2>
</div>
<div id="TOC">
<ul>
<li><a href="#the-classification-setting-again">The classification setting again</a></li>
<li><a href="#nearest-centroids">Nearest Centroids</a></li>
<li><a href="#the-optimal-rule">The Optimal Rule</a></li>
<li><a href="#the-gene-expression-model">The gene expression model</a></li>
<li><a href="#using-bayes-rule">Using Bayes Rule</a></li>
</ul>
</div>
<p>In this note we finalize our discussion about classification by introducing the Naive Bayes classifier as a probabilistic version of the nearest centroid classifier you are using in Project IV.</p>
<h3 id="the-classification-setting-again"><a href="#the-classification-setting-again">The classification setting again</a></h3>
<p>The setting is that we observe for each sample <span class="math">\(i\)</span> a set of <span class="math">\(p\)</span> features (gene expression in our case) represented in vector <span class="math">\(x_i\)</span>, and <em>qualitative</em> outcomes (or classes) <span class="math">\(g_i\)</span>, which can take values from a discrete set <span class="math">\(G\)</span>. In our case where our features are genomic measurements (gene expression for now), and that we have many more features than samples (i.e. <span class="math">\(p\)</span> &lt;&lt; <span class="math">\(N\)</span>, where <span class="math">\(N\)</span> is the number of samples). For gene expression, features are real numbers, and we can think of feature space as Euclidean space.</p>
<p>We want to learn a set of decision functions <span class="math">\(f_g\)</span> for each <span class="math">\(g \in G\)</span> and classify sample <span class="math">\(x_i\)</span> according to the rule</p>
<p><span class="math">\[
\hat{g}_i = \argmax_g f_g(x_i$
\]</span></p>
<p>We want to learn these functions to minimize <em>expected error rate</em>:</p>
<p>$$ E_{X,G} {<em>i g</em>i}</p>
<h3 id="nearest-centroids"><a href="#nearest-centroids">Nearest Centroids</a></h3>
<p>The nearest centroid classifier you are implementing in Project IV sets the decision functions using Euclidean distance. In which case <span class="math">\(f_g(x_i) = -\mathrm{dist}(x_i,\mu_g)=\|x_i - \mu_g\|^2\)</span>, where <span class="math">\(\mu_g\)</span> is the centroid learned for class <span class="math">\(g\)</span>. The classification is then choosing the nearest centroid.</p>
<p>Like soft k-means we can use a <em>soft-max</em> function here, and instead of using Euclidean distance directly, we can use a exponentially decaying function, normalized so it is a probability function.</p>
<p><span class="math">\[
f_g(x_i) = \frac{\exp \{- \|x_i - \mu_g \|^2 \}}{\sum_g \exp \{-\|x_i - \mu_g \|^2 \}}
\]</span></p>
<h3 id="the-optimal-rule"><a href="#the-optimal-rule">The Optimal Rule</a></h3>
<p>Although we won't show it, the best choice for <span class="math">\(f_g(x_i)\)</span> to minimize expected error rate is the <em>posterior class probability</em> <span class="math">\(P(G=g | x)\)</span>. Of course, we don't know this probability function, but the <em>soft-max</em> function we discussed above suggests a model for this posterior probability. We can derive it using Bayes Rule and assuming a probabilistic model for gene expression in each class <span class="math">\(g\)</span>.</p>
<h3 id="the-gene-expression-model"><a href="#the-gene-expression-model">The gene expression model</a></h3>
<p>Let's assume a normal distribution model for the expression of gene <span class="math">\(j\)</span> for samples in class <span class="math">\(g\)</span>. Recall that the normal distribution assigns probability to a given point <span class="math">\(x_{ij}\)</span> (e.g., expression for gene <span class="math">\(j\)</span> sample <span class="math">\(i\)</span>) using an exponentially decaying function of the point's distance to the mean<span class="math">\(\mu_{gj}\)</span>, scaled by parameter <span class="math">\(\sigma_j\)</span> (we will assume this parameter is the same for every class <span class="math">\(g\)</span>):</p>
<p><span class="math">\[
p_{gj}(x_{ij}) \sim \exp \{ - \frac{1}{2\sigma_j^2} (x_{ij} - \mu{gj})^2 \}
\]</span></p>
<p>The next assumption we'll make is that distribution of gene expression is independent across genes. Thus the expression for gene <span class="math">\(j\)</span> does not affect the expression of gene <span class="math">\(k\)</span> and viceversa for any pair of genes <span class="math">\(j\)</span> and <span class="math">\(k\)</span>. We know that biologically this is an awful assumption, but we'll make it for simplicity's sake (there are other reasons about overfitting, etc., which we're not getting into here).</p>
<p>With both of those assumptions we can write a full probability model for gene expression of class <span class="math">\(g\)</span>:</p>
<p><span class="math">\[
p_g(x_i) \sim \prod_{j} \exp \{ - \frac{1}{2\sigma_j^2} (x_{ij} - \mu{gj})^2 \}
\]</span></p>
<p>Notice that if <span class="math">\(\sigma_j^2=1\)</span> for all genes <span class="math">\(j\)</span> then the log probability of this model is a function of (negative) squared Euclidean distance:</p>
<p><span class="math">\[
\log p_g(x_i) \sim - \sum_j (x_{ij}-\mu{gj})^2 = - \|x_i - \mu_g \|^2
\]</span></p>
<h3 id="using-bayes-rule"><a href="#using-bayes-rule">Using Bayes Rule</a></h3>
<p>We can now use Bayes' Rule to derive a formula for the <em>posterior probability</em> model we need:</p>
<p><span class="math">\[
P(G=g | x_i) = \frac{p_g(x_i) P(G=g)}{\sum_k p_k(x_i) P(G=k)}
\]</span></p>
<p>where <span class="math">\(P(G=g)\)</span> is the <em>prior</em> probability of observing a sample from class <span class="math">\(g\)</span>. In the Alzheimer's example, <span class="math">\(P(G=\mathrm{severe})\)</span> is the <em>prior</em> probability of observing a sample with severe Alzheimer's.</p>
<p>Consider the case where we're only classifying between two classes, say normal and severe. The optimal rule (as we saw above) is to classify sample <span class="math">\(x_i\)</span> as severe when <span class="math">\(P(G=\mathrm{severe}|x_i) &gt; P(G=\mathrm{normal}|x_i)\)</span>. Using Bayes Rule we find that</p>
<p><span class="math">\[
\begin{eqnarray}
P(G=\mathrm{severe}|x_i) &amp; &gt; &amp;  P(G=\mathrm{normal}) \Leftrightarrow \\
\log \frac{P(G=\mathrm{severe}|x_i)}{P(G=\mathrm{normal})} &amp; &gt; &amp; 0 \Leftrightarrow \\
\log \frac{p_{\mathrm{severe}}(x_i)}{p_{\mathrm{normal}}(x_i) &amp; &gt; &amp; \log \frac{P(G=\mathrm{normal})}{P(G=\mathrm{severe})} 
\end{eqnarray}
\]</span></p>
<p>We can interpret the last line as &quot;we should predict severe when the evidence we get from gene expression outweighs our prior belief&quot;.</p>
<p>Finally, under the assumptions of independence and equi-variance <span class="math">\((\sigma_j^2=1\)</span> for all <span class="math">\(j\)</span>, we see that for the nearest centroid classifier this corresponds to biasing the distance calculation. I.e., classify severe when:</p>
<p><span class="math">\[
dist(x_i,\mu_{\mathrm{normal}}) - dist(x_i,\mu_{\mathrm{severe}}) &gt; \log \frac{P(G=\mathrm{normal})}{P(G=\mathrm{severe})}.
\]</span></p>
<p>We get the nearest centroid classifer again when the prior probabilities are equal and thus <span class="math">\(\log \frac{P(G=\mathrm{normal})}{P(G=\mathrm{severe})} = 0\)</span>.</p>
<p>Using the Bayes Rule here allows us to refine our classification rule based on the prior probability of observing samples from a given class.</p>
</body>
</html>
